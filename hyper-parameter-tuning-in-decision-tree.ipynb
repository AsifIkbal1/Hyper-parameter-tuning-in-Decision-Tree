{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1 style=\"color:brown\"> Basic Information about Decision Tree Classifier </h1>\n\n   **Decision Tree is a type of Supervised Machine Learning where data is splitted according to parameters in the features and patterns are learned to perform classification on unknown data with same kind of distribution (Test data).**\n   \n When applying decision tree with default hyper parameters, it splits until all patterns are learned in the training set. So that, tree get splited to N number of nodes and N number of depth. This leads to ***overfitting***. In other words, ***low bias and high variance***. Before going forward for hyper parameter tuning, familiarity of these terms is crucial. Let's get breif understanding.\n***\n* **Bias** - Performance of ML model on Training dataset (Known dataset). If models performs better on training data then bias of the model is `low`; otherwise, `High`\n* **Variance** - Performance of ML model on Test dataset (Unknown dataset). If model performs better on test data then variance of the model is `low`; otherwise, `High`\n\nIn ideal condition, we need `low bias and low variance`. That means, model should perform well on both training(known) data and testing(unkown) data \n\n**Overfitting** - It is a situation when model gets trained so well on training set; that it over-learns patterns in training data and it can not generalize well on new(unkown) data. Hence, it does not perform better on testing data. In other words, overfitting is a situation of `low bias and high variance`.\n\nIn practical, overfitting can be identified by checking ***Train Accuracy and Test Accuracy***. If difference between training accuracy and testing accuracy is high, it indicates model is overfitting and will not perform well on unknown data. So, we will try to minimize this difference to overcome from overfitting and to make model perform well on unkown data.\n\n> **By default, due to the nature of decision tree, it splits to N number of nodes and N number of depth which cause overfitting. That's why, Hyper-parameter tuning is necessary. So let's try to tune Decision Tree by taking example of Autism Dataset**","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-success\">\n <b>Tip:</b> I would strongly recommend to go through the videos and article mentioned in the reference section below to understand theory behind Decision Tree and some of the terms mentioned in this notebook like entropy and gini impurity; doing so will help to understand steps taken in this notebook more.\n</div>","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:03:34.833896Z","iopub.execute_input":"2022-06-09T03:03:34.834502Z","iopub.status.idle":"2022-06-09T03:03:35.593775Z","shell.execute_reply.started":"2022-06-09T03:03:34.83438Z","shell.execute_reply":"2022-06-09T03:03:35.593034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv('../input/autism/train.csv')","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:03:35.595623Z","iopub.execute_input":"2022-06-09T03:03:35.596332Z","iopub.status.idle":"2022-06-09T03:03:35.621436Z","shell.execute_reply.started":"2022-06-09T03:03:35.59629Z","shell.execute_reply":"2022-06-09T03:03:35.620162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Predict whether a person has autism or not; using other dependent variables (Features)**\n\n\n* `ID` - ID of the patient\n* `A1_Score to A10_Score` - Score based on Autism Spectrum Quotient (AQ) 10 item screening tool\n* `age` - Age of the patient in years\n* `gender` - Gender of the patient\n* `ethnicity` - Ethnicity of the patient\n* `jaundice` - Whether the patient had jaundice at the time of birth\n* `autism` - Whether an immediate family member has been diagnosed with autism\n* `contry_of_res` - Country of residence of the patient\n* `used_app_before` - Whether the patient has undergone a screening test before\n* `result` - Score for AQ1-10 screening test\n* `age_desc` - Age of the patient\n* `relation` - Relation of patient who completed the test\n* `Class/ASD` - Classified result as 0 or 1. Here 0 represents No and 1 represents Yes. This is the target column","metadata":{}},{"cell_type":"code","source":"df_train.info()","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:03:35.622901Z","iopub.execute_input":"2022-06-09T03:03:35.623632Z","iopub.status.idle":"2022-06-09T03:03:35.654619Z","shell.execute_reply.started":"2022-06-09T03:03:35.623587Z","shell.execute_reply":"2022-06-09T03:03:35.653701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:03:35.656253Z","iopub.execute_input":"2022-06-09T03:03:35.656975Z","iopub.status.idle":"2022-06-09T03:03:35.692002Z","shell.execute_reply.started":"2022-06-09T03:03:35.656911Z","shell.execute_reply":"2022-06-09T03:03:35.691023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(df_train.isnull(), cbar=False, cmap='viridis', yticklabels=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:03:35.694462Z","iopub.execute_input":"2022-06-09T03:03:35.695161Z","iopub.status.idle":"2022-06-09T03:03:36.042924Z","shell.execute_reply.started":"2022-06-09T03:03:35.695116Z","shell.execute_reply":"2022-06-09T03:03:36.041963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Above heatmap, does not contains any strips with different color which show that columns does not contain any NaN values. But we still need to tackle for uknown values.**","metadata":{}},{"cell_type":"code","source":"plt.pie(df_train['Class/ASD'].value_counts(), labels=[0,1], autopct='%1.1f%%', shadow=True)\n        \n#draw a circle at the center of pie to make it look like a donut\ncentre_circle = plt.Circle((0,0),0.75,facecolor='white')\nfig = plt.gcf()\nfig.gca().add_artist(centre_circle)\n\n\n# Set aspect ratio to be equal so that pie is drawn as a circle.\nplt.axis('equal')\nplt.show()  ","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:03:36.047283Z","iopub.execute_input":"2022-06-09T03:03:36.052245Z","iopub.status.idle":"2022-06-09T03:03:36.197167Z","shell.execute_reply.started":"2022-06-09T03:03:36.052192Z","shell.execute_reply":"2022-06-09T03:03:36.196057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**It can be noticed from the above chart that, almost 80% instances are of label 0 [Autism = No] and 20% instances in the dataset are label 1 [Autism = Yes]**","metadata":{}},{"cell_type":"code","source":"df_train['Class/ASD'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:03:36.198687Z","iopub.execute_input":"2022-06-09T03:03:36.199869Z","iopub.status.idle":"2022-06-09T03:03:36.211856Z","shell.execute_reply.started":"2022-06-09T03:03:36.199821Z","shell.execute_reply":"2022-06-09T03:03:36.210922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(6,6))\nax = sns.countplot(x='Class/ASD', data = df_train)\nfor p in ax.patches:\n   ax.annotate('{:.1f}'.format(p.get_height()), (p.get_x()+0.25, p.get_height()+0.01))","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:03:36.213546Z","iopub.execute_input":"2022-06-09T03:03:36.214238Z","iopub.status.idle":"2022-06-09T03:03:36.397112Z","shell.execute_reply.started":"2022-06-09T03:03:36.214192Z","shell.execute_reply":"2022-06-09T03:03:36.396167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### This shows imbalanced dataset","metadata":{}},{"cell_type":"code","source":"df_train['gender'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:03:36.398896Z","iopub.execute_input":"2022-06-09T03:03:36.399587Z","iopub.status.idle":"2022-06-09T03:03:36.409267Z","shell.execute_reply.started":"2022-06-09T03:03:36.399544Z","shell.execute_reply":"2022-06-09T03:03:36.408291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ax = sns.countplot(x='gender', data=df_train, hue='Class/ASD')\n\nfor p in ax.patches:\n   ax.annotate('{:.1f}'.format(p.get_height()), (p.get_x()+0.25, p.get_height()+0.01))","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:03:36.412267Z","iopub.execute_input":"2022-06-09T03:03:36.412663Z","iopub.status.idle":"2022-06-09T03:03:36.624701Z","shell.execute_reply.started":"2022-06-09T03:03:36.412633Z","shell.execute_reply":"2022-06-09T03:03:36.623861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Instanced in the dataset has Total 270 females and 530 males From the above bar chart, it can be inferred that 20 % of total female has Autism and 20 % total male has Autism.**","metadata":{}},{"cell_type":"code","source":"print('Unique categories in categorical columns : -')\nfor col in df_train.columns:\n    print(col,':')\n    n = df_train[col].nunique()\n    \n    if n < 25:\n        print(df_train[col].unique())","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-06-09T03:03:36.6264Z","iopub.execute_input":"2022-06-09T03:03:36.626829Z","iopub.status.idle":"2022-06-09T03:03:36.640619Z","shell.execute_reply.started":"2022-06-09T03:03:36.626789Z","shell.execute_reply":"2022-06-09T03:03:36.639688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 'age_desc' column has only one value - ['18 and more']\n# 'ID' column has all unique values and acting like an index. Hence, it can be removed\n\ndf_train.drop(['age_desc','ID'], axis=1, inplace= True)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:03:36.641653Z","iopub.execute_input":"2022-06-09T03:03:36.641972Z","iopub.status.idle":"2022-06-09T03:03:36.647339Z","shell.execute_reply.started":"2022-06-09T03:03:36.641926Z","shell.execute_reply":"2022-06-09T03:03:36.646475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(1,2, figsize=(12,6))\nsns.distplot(df_train['result'], ax= axes[0],bins=30, kde=False)\nsns.distplot(df_train[df_train['Class/ASD'] == 1]['result'], ax= axes[1],bins=30, kde=False)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:03:36.648364Z","iopub.execute_input":"2022-06-09T03:03:36.648648Z","iopub.status.idle":"2022-06-09T03:03:37.0608Z","shell.execute_reply.started":"2022-06-09T03:03:36.648622Z","shell.execute_reply":"2022-06-09T03:03:37.059797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Two histograms above shows frequency of values in two continuous columns - 'result' and 'age'.**\n\n**Histogram on right shows frquency of ages of people that has Autism. it shows that people with age 25-35 has highest number of Autism**\n\n**Histogram on left shows, what values of Result column has high chances of Autism. it shows, when the result value is between 11-14, the chances of having autism is higher**\n\nResult - Score for AQ1-10 Screening test","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.scatterplot(x='age',y='result', data=df_train, hue='Class/ASD')","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:03:37.06185Z","iopub.execute_input":"2022-06-09T03:03:37.06216Z","iopub.status.idle":"2022-06-09T03:03:37.345858Z","shell.execute_reply.started":"2022-06-09T03:03:37.062133Z","shell.execute_reply":"2022-06-09T03:03:37.344915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,8))\n\nsns.heatmap(df_train.corr(), annot= True, cmap='coolwarm')","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:03:37.347302Z","iopub.execute_input":"2022-06-09T03:03:37.347846Z","iopub.status.idle":"2022-06-09T03:03:38.276168Z","shell.execute_reply.started":"2022-06-09T03:03:37.347798Z","shell.execute_reply":"2022-06-09T03:03:38.275262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Above heatmap, show correlation of among columns. correlation values are between -1 to 1.\n\n* Value near to 1 = higher positive correlation between both columns\n* Value near to -1 = high negative correlation between both columns\n* value near to 0 = less correlation between both columns","metadata":{}},{"cell_type":"code","source":"df_train.describe()","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:03:38.277322Z","iopub.execute_input":"2022-06-09T03:03:38.277632Z","iopub.status.idle":"2022-06-09T03:03:38.329Z","shell.execute_reply.started":"2022-06-09T03:03:38.277604Z","shell.execute_reply":"2022-06-09T03:03:38.328017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Handling Outliers for continuous columns","metadata":{}},{"cell_type":"markdown","source":"1. **Age Column**","metadata":{}},{"cell_type":"code","source":"n = 0\nfor i in df_train['age']:\n    q1 = df_train['age'].quantile(0.25)\n    q3 = df_train['age'].quantile(0.75)\n    iqr = q3 - q1\n    upper_tail = q3 + 1.5 * iqr\n    lower_tail = q1 - 1.5 * iqr\n    \n    if i > upper_tail or i < lower_tail:\n           n = n+1\n            \nprint('Total number of outliers in Age column: ',n)","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:03:38.333069Z","iopub.execute_input":"2022-06-09T03:03:38.333392Z","iopub.status.idle":"2022-06-09T03:03:39.457697Z","shell.execute_reply.started":"2022-06-09T03:03:38.333363Z","shell.execute_reply":"2022-06-09T03:03:39.456743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.boxplot(x='age', data= df_train)\nplt.title('Age column before handling outliers')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:03:39.458987Z","iopub.execute_input":"2022-06-09T03:03:39.459271Z","iopub.status.idle":"2022-06-09T03:03:39.611619Z","shell.execute_reply.started":"2022-06-09T03:03:39.459245Z","shell.execute_reply":"2022-06-09T03:03:39.610416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**From boxplot above, it can be noticed that there are few outliers from 60 to 80 in Age column**","metadata":{}},{"cell_type":"code","source":"#Replacing uper outliers with upper_tail (q3 + 1.5 * iqr) of Age column\n#Replacing lower outliers with mean of Age column\nfor i in df_train['age']:\n    q1 = df_train['age'].quantile(0.25)\n    q3 = df_train['age'].quantile(0.75)\n    \n    \n    iqr = q3 - q1\n    upper_tail = q3 + 1.5 * iqr\n    lower_tail = q1 - 1.5 * iqr\n    \n    if i > upper_tail or i < lower_tail:\n        if i> upper_tail:\n            df_train['age'] = df_train['age'].replace(i, upper_tail)\n        else:\n            df_train['age'] = df_train['age'].replace(i, np.mean(i))\n\nsns.boxplot(x='age', data=df_train)\nplt.title('Age column after handling outliers')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:03:39.612984Z","iopub.execute_input":"2022-06-09T03:03:39.613361Z","iopub.status.idle":"2022-06-09T03:03:40.871766Z","shell.execute_reply.started":"2022-06-09T03:03:39.613333Z","shell.execute_reply":"2022-06-09T03:03:40.870969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2. **'Result' Column**","metadata":{}},{"cell_type":"code","source":"n = 0\nfor i in df_train['result']:\n    q1 = df_train['result'].quantile(0.25)\n    q3 = df_train['result'].quantile(0.75)\n    iqr = q3 - q1\n    upper_tail = q3 + 1.5 * iqr\n    lower_tail = q1 - 1.5 * iqr\n    \n\n    if i > upper_tail or i < lower_tail:\n        n = n+1\n                        \nprint('Total number of outliers in result column: ',n)","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:03:40.87331Z","iopub.execute_input":"2022-06-09T03:03:40.873718Z","iopub.status.idle":"2022-06-09T03:03:41.997487Z","shell.execute_reply.started":"2022-06-09T03:03:40.873678Z","shell.execute_reply":"2022-06-09T03:03:41.996557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.boxplot(x='result', data= df_train)\nplt.title('Result column before handling outlier')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:03:41.998784Z","iopub.execute_input":"2022-06-09T03:03:41.999119Z","iopub.status.idle":"2022-06-09T03:03:42.140145Z","shell.execute_reply.started":"2022-06-09T03:03:41.99909Z","shell.execute_reply":"2022-06-09T03:03:42.13902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Replacing upper outlier with 90th percentile of 'result' column (If any).\n#Replacing lower outlier with 10th percentile of 'result' column (If any).\nfor i in df_train['result']:\n    q1 = df_train['result'].quantile(0.25)\n    q3 = df_train['result'].quantile(0.75)\n    iqr = q3 - q1\n    upper_tail = q3 + 1.5 * iqr\n    lower_tail = q1 - 1.5 * iqr\n\n    if i > upper_tail or i < lower_tail:\n        if i < lower_tail:\n            df_train['result'] = df_train['result'].replace(i,df_train['result'].quantile(0.10) )\n        else:\n            df_train['result'] = df_train['result'].replace(i,df_train['result'].quantile(0.90) )\n    \nsns.boxplot(x='result', data=df_train)\nplt.title('Result column after handling outliers')\nplt.show()      ","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:03:42.141975Z","iopub.execute_input":"2022-06-09T03:03:42.142524Z","iopub.status.idle":"2022-06-09T03:03:43.402282Z","shell.execute_reply.started":"2022-06-09T03:03:42.142476Z","shell.execute_reply":"2022-06-09T03:03:43.40127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Handling Missing Values","metadata":{}},{"cell_type":"markdown","source":"1. **Ethnicity Column**","metadata":{}},{"cell_type":"code","source":"df_train['ethnicity'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:03:43.407545Z","iopub.execute_input":"2022-06-09T03:03:43.408449Z","iopub.status.idle":"2022-06-09T03:03:43.417794Z","shell.execute_reply.started":"2022-06-09T03:03:43.408406Z","shell.execute_reply":"2022-06-09T03:03:43.417029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['ethnicity'] = df_train['ethnicity'].replace(['?','others'],['Others','Others'])","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:03:43.419196Z","iopub.execute_input":"2022-06-09T03:03:43.419875Z","iopub.status.idle":"2022-06-09T03:03:43.426713Z","shell.execute_reply.started":"2022-06-09T03:03:43.419839Z","shell.execute_reply":"2022-06-09T03:03:43.426008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Above, replaced '?' and 'others' with 'Others' in 'ethnicity column'**","metadata":{}},{"cell_type":"code","source":"df_train['ethnicity'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:03:43.427578Z","iopub.execute_input":"2022-06-09T03:03:43.428055Z","iopub.status.idle":"2022-06-09T03:03:43.439608Z","shell.execute_reply.started":"2022-06-09T03:03:43.428026Z","shell.execute_reply":"2022-06-09T03:03:43.438739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2. **Relation column**","metadata":{}},{"cell_type":"code","source":"df_train['relation'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:03:43.440743Z","iopub.execute_input":"2022-06-09T03:03:43.44166Z","iopub.status.idle":"2022-06-09T03:03:43.449683Z","shell.execute_reply.started":"2022-06-09T03:03:43.441615Z","shell.execute_reply":"2022-06-09T03:03:43.449005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['relation'] = df_train['relation'].replace('?','Others')","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:03:43.450691Z","iopub.execute_input":"2022-06-09T03:03:43.451083Z","iopub.status.idle":"2022-06-09T03:03:43.459639Z","shell.execute_reply.started":"2022-06-09T03:03:43.451051Z","shell.execute_reply":"2022-06-09T03:03:43.458957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Above, replaced '?' with 'Others' in 'Relation' column**","metadata":{}},{"cell_type":"markdown","source":"3. **Country_of_res**","metadata":{}},{"cell_type":"code","source":"df_train['contry_of_res'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:03:43.460838Z","iopub.execute_input":"2022-06-09T03:03:43.461735Z","iopub.status.idle":"2022-06-09T03:03:43.472511Z","shell.execute_reply.started":"2022-06-09T03:03:43.461702Z","shell.execute_reply":"2022-06-09T03:03:43.471931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Merging coutnries which are occurring less then 6 times into 'Other' category.**","metadata":{}},{"cell_type":"code","source":"occ = df_train['contry_of_res'].value_counts()\nli_country = []\nfor i in occ.index:\n    if occ[i] < 6:\n        li_country.append(i)\nprint(li_country)","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:03:43.473716Z","iopub.execute_input":"2022-06-09T03:03:43.474304Z","iopub.status.idle":"2022-06-09T03:03:43.485355Z","shell.execute_reply.started":"2022-06-09T03:03:43.474261Z","shell.execute_reply":"2022-06-09T03:03:43.484488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.replace(li_country, 'Others', inplace= True)","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:03:43.486575Z","iopub.execute_input":"2022-06-09T03:03:43.486869Z","iopub.status.idle":"2022-06-09T03:03:43.508968Z","shell.execute_reply.started":"2022-06-09T03:03:43.486842Z","shell.execute_reply":"2022-06-09T03:03:43.508063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['contry_of_res'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:03:43.510019Z","iopub.execute_input":"2022-06-09T03:03:43.510679Z","iopub.status.idle":"2022-06-09T03:03:43.518151Z","shell.execute_reply.started":"2022-06-09T03:03:43.510647Z","shell.execute_reply":"2022-06-09T03:03:43.517526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Seperating features and Target","metadata":{}},{"cell_type":"code","source":"train_feats = df_train.drop('Class/ASD', axis=1)\ntrain_target = df_train['Class/ASD']","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:03:43.519344Z","iopub.execute_input":"2022-06-09T03:03:43.519798Z","iopub.status.idle":"2022-06-09T03:03:43.657394Z","shell.execute_reply.started":"2022-06-09T03:03:43.519769Z","shell.execute_reply":"2022-06-09T03:03:43.656448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_feats.shape, train_target.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:03:43.659091Z","iopub.execute_input":"2022-06-09T03:03:43.659591Z","iopub.status.idle":"2022-06-09T03:03:43.6708Z","shell.execute_reply.started":"2022-06-09T03:03:43.659543Z","shell.execute_reply":"2022-06-09T03:03:43.669999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Creating two dataframes - String dataframe , Numeric dataframe","metadata":{}},{"cell_type":"code","source":"train_cat = train_feats.select_dtypes(include='object')\ntrain_num = train_feats.select_dtypes(exclude='object')","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:03:43.672023Z","iopub.execute_input":"2022-06-09T03:03:43.672653Z","iopub.status.idle":"2022-06-09T03:03:43.683681Z","shell.execute_reply.started":"2022-06-09T03:03:43.672618Z","shell.execute_reply":"2022-06-09T03:03:43.682837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_cat.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:03:43.684996Z","iopub.execute_input":"2022-06-09T03:03:43.685299Z","iopub.status.idle":"2022-06-09T03:03:43.702185Z","shell.execute_reply.started":"2022-06-09T03:03:43.685272Z","shell.execute_reply":"2022-06-09T03:03:43.70123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Handling Categorical columns","metadata":{}},{"cell_type":"markdown","source":"**Machine Learning models can not understand input in String format. Therefore, we need to convert columns with String values into numeric representation. For that reason, We are using OneHotEncoding**","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:03:43.703113Z","iopub.execute_input":"2022-06-09T03:03:43.703719Z","iopub.status.idle":"2022-06-09T03:03:43.765658Z","shell.execute_reply.started":"2022-06-09T03:03:43.703686Z","shell.execute_reply":"2022-06-09T03:03:43.764749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Specified handle_unknown = 'ignore' because many times test dataset contains categories that model/transformer does not encountered in traing set while training.\n# Therefore, it handles new categories encountered while transforming testing dataset.\nenc = OneHotEncoder(handle_unknown = 'ignore')\n\ntrain_cat_enc = enc.fit_transform(train_cat)","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:03:43.770705Z","iopub.execute_input":"2022-06-09T03:03:43.771308Z","iopub.status.idle":"2022-06-09T03:03:43.783495Z","shell.execute_reply.started":"2022-06-09T03:03:43.771273Z","shell.execute_reply":"2022-06-09T03:03:43.782531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_cat_enc.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:03:43.785004Z","iopub.execute_input":"2022-06-09T03:03:43.785877Z","iopub.status.idle":"2022-06-09T03:03:43.792344Z","shell.execute_reply.started":"2022-06-09T03:03:43.785834Z","shell.execute_reply":"2022-06-09T03:03:43.791575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_cat = pd.DataFrame(train_cat_enc.toarray())\n","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:03:43.79342Z","iopub.execute_input":"2022-06-09T03:03:43.793834Z","iopub.status.idle":"2022-06-09T03:03:43.803556Z","shell.execute_reply.started":"2022-06-09T03:03:43.793806Z","shell.execute_reply":"2022-06-09T03:03:43.802654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_cat.head()\n# Converted String columns into numeric format","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:03:43.804981Z","iopub.execute_input":"2022-06-09T03:03:43.805654Z","iopub.status.idle":"2022-06-09T03:03:43.844224Z","shell.execute_reply.started":"2022-06-09T03:03:43.80562Z","shell.execute_reply":"2022-06-09T03:03:43.843578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Scaling Numeric Continuous columns","metadata":{}},{"cell_type":"markdown","source":"**Many times in Machine learning, some features contain high range of values and other features contain lower range of values. In this case, ML model gives more importance to features with high range of values and neglect features which has lower range of values even if these features are important. Hence, Scaling converts values of all features into same lower range which leads to optimal training and ML model can converge easily and efficiently**","metadata":{}},{"cell_type":"markdown","source":"**We are using MinMaxScaler that converts values of given features from 0 to 1.**","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:03:43.845611Z","iopub.execute_input":"2022-06-09T03:03:43.846053Z","iopub.status.idle":"2022-06-09T03:03:43.850098Z","shell.execute_reply.started":"2022-06-09T03:03:43.846024Z","shell.execute_reply":"2022-06-09T03:03:43.848991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = MinMaxScaler()\n\ntrain_scaled = scaler.fit_transform(train_num[['age','result']])","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:03:43.851402Z","iopub.execute_input":"2022-06-09T03:03:43.852027Z","iopub.status.idle":"2022-06-09T03:03:43.864369Z","shell.execute_reply.started":"2022-06-09T03:03:43.851986Z","shell.execute_reply":"2022-06-09T03:03:43.863514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_scaled = pd.DataFrame(train_scaled, columns=['age','result'])","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:03:43.865461Z","iopub.execute_input":"2022-06-09T03:03:43.865783Z","iopub.status.idle":"2022-06-09T03:03:43.874886Z","shell.execute_reply.started":"2022-06-09T03:03:43.865748Z","shell.execute_reply":"2022-06-09T03:03:43.87411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_scaled.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:03:43.876053Z","iopub.execute_input":"2022-06-09T03:03:43.876602Z","iopub.status.idle":"2022-06-09T03:03:43.889912Z","shell.execute_reply.started":"2022-06-09T03:03:43.87657Z","shell.execute_reply":"2022-06-09T03:03:43.889276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Above dataframe shows that, values of age and result have been convert between 0 to 1**","metadata":{}},{"cell_type":"markdown","source":"### Merging dataframes together","metadata":{}},{"cell_type":"code","source":"train_num.drop(['age','result'], axis=1, inplace= True) # Removing old columns","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:03:43.890774Z","iopub.execute_input":"2022-06-09T03:03:43.891472Z","iopub.status.idle":"2022-06-09T03:03:43.906065Z","shell.execute_reply.started":"2022-06-09T03:03:43.891443Z","shell.execute_reply":"2022-06-09T03:03:43.904505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_num = pd.concat([train_num,df_train_scaled], axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:03:43.907559Z","iopub.execute_input":"2022-06-09T03:03:43.908153Z","iopub.status.idle":"2022-06-09T03:03:43.913518Z","shell.execute_reply.started":"2022-06-09T03:03:43.908113Z","shell.execute_reply":"2022-06-09T03:03:43.912648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_num.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:03:43.914763Z","iopub.execute_input":"2022-06-09T03:03:43.915337Z","iopub.status.idle":"2022-06-09T03:03:43.933982Z","shell.execute_reply.started":"2022-06-09T03:03:43.915297Z","shell.execute_reply":"2022-06-09T03:03:43.933147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Merging categorical dataframe and numerical dataframe below**","metadata":{}},{"cell_type":"code","source":"df_train_final = pd.concat([df_train_num,df_train_cat], axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:03:43.935212Z","iopub.execute_input":"2022-06-09T03:03:43.935613Z","iopub.status.idle":"2022-06-09T03:03:43.942012Z","shell.execute_reply.started":"2022-06-09T03:03:43.935573Z","shell.execute_reply":"2022-06-09T03:03:43.940786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_final.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:03:43.943276Z","iopub.execute_input":"2022-06-09T03:03:43.943858Z","iopub.status.idle":"2022-06-09T03:03:43.952577Z","shell.execute_reply.started":"2022-06-09T03:03:43.943817Z","shell.execute_reply":"2022-06-09T03:03:43.951935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Convert datatype of column name from int to String in order to avoid warnings**","metadata":{}},{"cell_type":"code","source":"col_name = list(df_train_final.columns)\ncol_name = [str(name) for name in col_name]\ndf_train_final.columns = col_name\nprint(df_train_final.columns)","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:03:43.953657Z","iopub.execute_input":"2022-06-09T03:03:43.954631Z","iopub.status.idle":"2022-06-09T03:03:43.966115Z","shell.execute_reply.started":"2022-06-09T03:03:43.954589Z","shell.execute_reply":"2022-06-09T03:03:43.965251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Spliting Dataset into Train and test split","metadata":{}},{"cell_type":"markdown","source":"Splitting traing set and test test from 'df_train_final' dataframe","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:03:43.967364Z","iopub.execute_input":"2022-06-09T03:03:43.96791Z","iopub.status.idle":"2022-06-09T03:03:44.02801Z","shell.execute_reply.started":"2022-06-09T03:03:43.967868Z","shell.execute_reply":"2022-06-09T03:03:44.027259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(df_train_final, train_target, test_size=0.25, stratify=df_train['Class/ASD'], random_state=40)","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:03:44.029167Z","iopub.execute_input":"2022-06-09T03:03:44.029598Z","iopub.status.idle":"2022-06-09T03:03:44.038228Z","shell.execute_reply.started":"2022-06-09T03:03:44.02957Z","shell.execute_reply":"2022-06-09T03:03:44.037396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape, y_train.shape, X_test.shape, y_test.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:03:44.039358Z","iopub.execute_input":"2022-06-09T03:03:44.039799Z","iopub.status.idle":"2022-06-09T03:03:44.046738Z","shell.execute_reply.started":"2022-06-09T03:03:44.039761Z","shell.execute_reply":"2022-06-09T03:03:44.045936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Model Training - Decision Tree**","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:03:44.048056Z","iopub.execute_input":"2022-06-09T03:03:44.048772Z","iopub.status.idle":"2022-06-09T03:03:44.196378Z","shell.execute_reply.started":"2022-06-09T03:03:44.048715Z","shell.execute_reply":"2022-06-09T03:03:44.195602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- ### **Default Hyper-parameters**","metadata":{}},{"cell_type":"code","source":"dt = DecisionTreeClassifier(random_state=42)\n#dt = DecisionTreeClassifier(criterion='gini', splitter='best',ccp_alpha= 0.01,random_state=42)\ndt.fit(X_train, y_train)\nprint('Train Accuracy: ', dt.score(X_train, y_train))\n\npred = dt.predict(X_test)\n\nprint('Test Accuracy: ', accuracy_score(pred, y_test))\nprint('f1 Score: ', f1_score(pred, y_test))\nprint('Confusion Matrix:\\n ', confusion_matrix(pred, y_test))","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:03:44.197484Z","iopub.execute_input":"2022-06-09T03:03:44.197813Z","iopub.status.idle":"2022-06-09T03:03:44.227894Z","shell.execute_reply.started":"2022-06-09T03:03:44.197783Z","shell.execute_reply":"2022-06-09T03:03:44.227243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-warning\">\n <p style= \"color:brown\"}> ‚¨ÜÔ∏èAt first, if we check Train Accuracy is 1.0, it indicates model is 100 % accurate on train data(known data) which indicates Low Bias. However, Test Accuracy is 0.78 which indicates model is 78 % accurate on test data (unkown data) which indicates High Variance. High Difference between Train and test Accuracy shows model is <b>Overfitting.</b> In other words, <b>low bias and high variance</b></p>\n</div>","metadata":{}},{"cell_type":"code","source":"#Plotting tree which is trained on Training dataset. \n#sklearn provides 'tree' library which helps to plot trained tree easily\n\nfrom sklearn import tree\nplt.figure(figsize=(20,10))\ntree.plot_tree(dt, filled=True)\nplt.title('Decision Tree when hyper-parameters are default')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:03:44.229084Z","iopub.execute_input":"2022-06-09T03:03:44.229539Z","iopub.status.idle":"2022-06-09T03:03:51.027648Z","shell.execute_reply.started":"2022-06-09T03:03:44.229508Z","shell.execute_reply":"2022-06-09T03:03:51.026961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style='color:brown'><b>‚¨ÜÔ∏èPlotted trained decision tree above. It can be seen that complex Tree has been created with default parameters even if our problem and dataset is not that much complex. Here, Decision Tree is trying to fit every single pattern of the training dataset and it is not trying to generalize. Therefore, it will not provide better results on unkown data. So, let's check some hyper parameter that can help us to tackle this issue.</b></p>","metadata":{}},{"cell_type":"markdown","source":"***Brief explaination of few hyper-parameters of decision tree (sk-learn)***\n\n* `max_depth` = It helps to restrict depth of the tree. If set to 2, then Tree will be splitted to depth 2 (level 0,1 and 2). If set to None, then nodes are expanded until all leaves are pure. Restricting depth of the tree will stop Decision Tree to make over splits and over-learn training data which will handle overfitting.\n\n* `max_samples_leaf` = We can specify minimum samples required to split node and create leaf nodes from it. For example, if it is set to 20, then splitting current node should have more than 20 samples in its child nodes. If not, then tree will not do split from that current node. This can again restrict size of the Tree and can help to handle overfitting.\n\n* `ccp_alpha` = ccp stands for Cost Complexity Pruning. Pruning is a process of cutting weak nodes(nodes where we have less confidence) from the tree. ccp_alpha is a powerful hyperparameter and setting it appropiately can improve model performance significantly. \n\n**High value of ccp_alpha shows high amount of pruning and low values of ccp_alpha shows less amount of pruning.**\n\n* `criterion` = This hyperparameter is also crucial and it does not help to handle overfitting. But, it helps to select order of features to split nodes on. Basically, we can calculate from several features which feature is best to split the current node on. This is important because we want to create a tree with smallest possible depth which is accurate as well as efficient due to small depth.\n\nThree values that can be specified in criterion are: <code style=\"background:yellow;color:black\">1) Gini 2) Entropy 3) log_loss</code>\n\n*By default, `Gini impurity` is set in criterion hyperparameter because gini impurity is efficient than `Entropy`. The main reason behind this is, formula of Entropy contains Log calcualtions which generally takes time and formula of gini impurity contains simple calculation without Log calculations*","metadata":{}},{"cell_type":"markdown","source":"> <span style=\"font-size:22px; color:green\"> Let's try these hyper-parameters one by one to check their individual impact.</span>","metadata":{}},{"cell_type":"markdown","source":"### 1.Hyper-parameter :  max_depth","metadata":{}},{"cell_type":"code","source":"#max_depth = 5\ndt = DecisionTreeClassifier(max_depth=5, random_state=42) #Now we are specifying max_depth at the time of creating object of DecisionTreeClassifier\ndt.fit(X_train, y_train)\nprint('Train Accuracy: ',dt.score(X_train, y_train))\n\npred = dt.predict(X_test)\n\nprint('Test Accuracy: ', accuracy_score(pred, y_test))\nprint('f1 Score: ', f1_score(pred, y_test))\nprint('Confusion Matrix:\\n ', confusion_matrix(pred, y_test))","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:03:51.028685Z","iopub.execute_input":"2022-06-09T03:03:51.029304Z","iopub.status.idle":"2022-06-09T03:03:51.049491Z","shell.execute_reply.started":"2022-06-09T03:03:51.029269Z","shell.execute_reply":"2022-06-09T03:03:51.048584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**‚¨ÜÔ∏èDifference between Train and Test Accuracy is decreased**\n\n*Let's check other value of max_depth*","metadata":{}},{"cell_type":"code","source":"#max_depth = 3\ndt = DecisionTreeClassifier(max_depth=3, random_state=42) #Now we are specifying max_depth at the time of creating object of DecisionTreeClassifier\ndt.fit(X_train, y_train)\nprint('Train Accuracy: ',dt.score(X_train, y_train))\n\npred = dt.predict(X_test)\n\nprint('Test Accuracy: ', accuracy_score(pred, y_test))\nprint('f1 Score: ', f1_score(pred, y_test))\nprint('Confusion Matrix:\\n ', confusion_matrix(pred, y_test))","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:03:51.050787Z","iopub.execute_input":"2022-06-09T03:03:51.051179Z","iopub.status.idle":"2022-06-09T03:03:51.069695Z","shell.execute_reply.started":"2022-06-09T03:03:51.051148Z","shell.execute_reply":"2022-06-09T03:03:51.068778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-warning\" style='color:brown'>‚¨ÜÔ∏è Now if we check train accuracy(0.86) and test accuracy (0.85), it can be inferred that difference between both accuracy is very less than before. Less differences shows model is not overfitting. Therefore, max_depth has handled overfitting.üëç</div>\n\n* <span style=\"font-size:17px;\"> Also notice that, train accuracy is decreased then first model. But test accuracy has been increased in this model that means, at a cost of performing bit less accurate on known data, model is performing better on unkown data. And, that's the main aim of Machine Learning models -> [peforming better on unkown or new data]. </span>\n\n* <span style=\"font-size:17px;\"> In other words, increased test accuracy shows this model achieved low variance. Train accuracy is also good enough; hence, we can consider bias is also not very high; but low enough. </span>","metadata":{}},{"cell_type":"code","source":"from sklearn import tree\nplt.figure(figsize=(20,10))\ntree.plot_tree(dt, filled=True)\nplt.title('Decision Tree when max_depth=3')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:03:51.072698Z","iopub.execute_input":"2022-06-09T03:03:51.073007Z","iopub.status.idle":"2022-06-09T03:03:51.787555Z","shell.execute_reply.started":"2022-06-09T03:03:51.072944Z","shell.execute_reply":"2022-06-09T03:03:51.786666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* <p style='color:brown'><b>‚¨ÜÔ∏èPlotted tree above for max_depth=3. Root node is Level 0 and then tree is splitted to level 1,2 and 3. This Decision Tree is less complex and it gave better f1 score and confusion matrix.</b></p>\n\n* <p style='color:brown'><b> F1-score is weighted average of Precision and Recall. F1-score is better performance measure than accuracy when classes in the Target are imbalanced.</b></p>","metadata":{}},{"cell_type":"markdown","source":"### 2.Hyper-parameter :  min_samples_leaf","metadata":{}},{"cell_type":"code","source":"\ndt = DecisionTreeClassifier(min_samples_leaf= 10, random_state=42) #Now we are specifying min_samples_leaf at the time of creating object of DecisionTreeClassifier\ndt.fit(X_train, y_train)\nprint('Train Accuracy: ',dt.score(X_train, y_train))\n\npred = dt.predict(X_test)\n\nprint('Test Accuracy: ', accuracy_score(pred, y_test))\nprint('f1 Score: ', f1_score(pred, y_test))\nprint('Confusion Matrix:\\n ', confusion_matrix(pred, y_test))","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:03:51.788681Z","iopub.execute_input":"2022-06-09T03:03:51.789025Z","iopub.status.idle":"2022-06-09T03:03:51.809397Z","shell.execute_reply.started":"2022-06-09T03:03:51.788992Z","shell.execute_reply":"2022-06-09T03:03:51.808664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**‚¨ÜÔ∏èDifference between Train and Test Accuracy is decreased. But it can be decreased more**\n\n*Let's try another value for min_samples_leaf*\n","metadata":{}},{"cell_type":"code","source":"dt = DecisionTreeClassifier(min_samples_leaf= 30, random_state=42) #Now we are specifying min_samples_leaf at the time of creating object of DecisionTreeClassifier\ndt.fit(X_train, y_train)\nprint('Train Accuracy: ',dt.score(X_train, y_train))\n\npred = dt.predict(X_test)\n\nprint('Test Accuracy: ', accuracy_score(pred, y_test))\nprint('f1 Score: ', f1_score(pred, y_test))\nprint('Confusion Matrix:\\n ', confusion_matrix(pred, y_test))","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:03:51.810425Z","iopub.execute_input":"2022-06-09T03:03:51.810737Z","iopub.status.idle":"2022-06-09T03:03:51.829613Z","shell.execute_reply.started":"2022-06-09T03:03:51.810709Z","shell.execute_reply":"2022-06-09T03:03:51.829003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* <span style=\"font-size:17px;\"> First, we kept `min_samples_leaf = 10`; that means, minimum 10 samples are required in child node to split current node and create leaf node from it. Then, we set `min_samples_leaf = 30`; that means, minimum 30 samples are required in child nodes to split current node and create leaf node from it. Increasing the value of min_samples_leaf limits the Decision tree more to split current nodes and create over child nodes. This way, over-training on train dataset can be restricted and overfitting can be handled. </span>\n\n* <span style=\"font-size:17px;\"> It can be seen that, when `min_samples_leaf =30`, difference between train and test accuracy is less and model is showing better result through `f1-score (0.68)` and balanced result on confusion matrix by performing well on both classes <b>[0->(No Autism) & 1-> (Yes Autism)].</b></span>","metadata":{}},{"cell_type":"code","source":"from sklearn import tree\nplt.figure(figsize=(20,10))\ntree.plot_tree(dt, filled=True)\nplt.title('Decision Tree when min_samples_leaf= 30')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:03:51.830493Z","iopub.execute_input":"2022-06-09T03:03:51.831404Z","iopub.status.idle":"2022-06-09T03:03:52.787453Z","shell.execute_reply.started":"2022-06-09T03:03:51.83136Z","shell.execute_reply":"2022-06-09T03:03:52.786817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style='color:brown'><b>‚¨ÜÔ∏è If we check leaf nodes of above plotted tree, samples in each leaf node are greater than 30 because we set hyper parameter min_samples_leaf = 30</b></p>","metadata":{}},{"cell_type":"markdown","source":"### 3.Hyper-parameter :  ccp_alpha (Cost Complexity Pruning)","metadata":{}},{"cell_type":"markdown","source":"* **Cost complexity pruning provides another option to control the size of a tree. In DecisionTreeClassifier, this pruning technique is parameterized by the cost complexity parameter, ccp_alpha. Greater values of ccp_alpha increase the number of nodes pruned.**\n\n* `cost_complexity_pruning_path`  **method of DecisionTreeClassifier returns the effective alphas out of which one best alpha can be selected.**","metadata":{}},{"cell_type":"code","source":"dt = DecisionTreeClassifier(random_state=42)\npath = dt.cost_complexity_pruning_path(X_train, y_train)\nccp_alphas, impurities = path.ccp_alphas , path.impurities","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:03:52.788474Z","iopub.execute_input":"2022-06-09T03:03:52.78893Z","iopub.status.idle":"2022-06-09T03:03:52.799892Z","shell.execute_reply.started":"2022-06-09T03:03:52.788901Z","shell.execute_reply":"2022-06-09T03:03:52.799269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(ccp_alphas) #effective values of alphas","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:03:52.800903Z","iopub.execute_input":"2022-06-09T03:03:52.801517Z","iopub.status.idle":"2022-06-09T03:03:52.808931Z","shell.execute_reply.started":"2022-06-09T03:03:52.801433Z","shell.execute_reply":"2022-06-09T03:03:52.808308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we will train multiple Decision Tree Models by setting ccp_alphas values from above array as hyper parameter of classifiers and append each model in list `dts`","metadata":{}},{"cell_type":"code","source":"dts = []\n\nfor ccp in ccp_alphas:\n    dt = DecisionTreeClassifier(ccp_alpha= ccp,random_state=42)\n    dt.fit(X_train, y_train)\n    dts.append(dt)","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:03:52.810016Z","iopub.execute_input":"2022-06-09T03:03:52.810462Z","iopub.status.idle":"2022-06-09T03:03:53.001521Z","shell.execute_reply.started":"2022-06-09T03:03:52.810434Z","shell.execute_reply":"2022-06-09T03:03:53.000683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Getting the Train Accuracy score and Test Accuracy Score of each model and saving scores in the corresponding lists**","metadata":{}},{"cell_type":"code","source":"train_scores = [dt.score(X_train, y_train) for dt in dts]\ntest_scores = [dt.score(X_test, y_test) for dt in dts]","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:03:53.002712Z","iopub.execute_input":"2022-06-09T03:03:53.003043Z","iopub.status.idle":"2022-06-09T03:03:53.16588Z","shell.execute_reply.started":"2022-06-09T03:03:53.003012Z","shell.execute_reply":"2022-06-09T03:03:53.16511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Accuracy Vs Alphas for training and test sets ","metadata":{}},{"cell_type":"code","source":"\nfig, ax = plt.subplots(figsize=(10,6))\nax.set_xlabel('ccp_alpha')\nax.set_ylabel('Accuracy')\nax.set_title('Accuracy Vs Alpha for train and test sets')\nax.plot(ccp_alphas, train_scores, marker='o', label='train', drawstyle='steps-post')\nax.plot(ccp_alphas, test_scores, marker='x', label='test',  drawstyle='steps-post')\nax.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:03:53.166879Z","iopub.execute_input":"2022-06-09T03:03:53.167177Z","iopub.status.idle":"2022-06-09T03:03:53.376992Z","shell.execute_reply.started":"2022-06-09T03:03:53.167151Z","shell.execute_reply":"2022-06-09T03:03:53.37593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***Above line graph shows relation between training-test accuracy and CCP_alphas (Cost Complexity Pruning)***\n\n1. <span style=\"font-size:16px;\">As value of CCP_alpha is increasing, difference between training accuracy and testing accuracy is decreasing.\nLess difference between both accuracy shows,  CCP_alpha is resolving the issue of overfitting by pruning the weak nodes of the Decision tree.</span>\n\n2. <span style=\"font-size:16px;\">It can be noticed that Decision Tree Classifier will generate optimal result by setting ccp_alpha value somewhere between 0.005 to 0.02 because Train accuracy and test accuracy is very near from each other and also bit higher than the accuracy when ccp_alpha is greater than 0.02.</span>\n","metadata":{}},{"cell_type":"markdown","source":"**Tried some ccp_alpha values between 0.005 and 0.02 and got best result on ccp_alpha=0.007**","metadata":{}},{"cell_type":"code","source":"dt = DecisionTreeClassifier(ccp_alpha= 0.007,random_state=42)\ndt.fit(X_train, y_train)\nprint('Training Accuracy: ',dt.score(X_train, y_train))\n\npred = dt.predict(X_test)\nprint('Test Accuracy: ', accuracy_score(pred, y_test))\nprint('f1 Score: ', f1_score(pred, y_test))\nprint('Confusion Matrix:\\n ', confusion_matrix(pred, y_test))","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:03:53.378012Z","iopub.execute_input":"2022-06-09T03:03:53.378326Z","iopub.status.idle":"2022-06-09T03:03:53.39972Z","shell.execute_reply.started":"2022-06-09T03:03:53.378297Z","shell.execute_reply":"2022-06-09T03:03:53.398832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import tree\nplt.figure(figsize=(20,10))\ntree.plot_tree(dt, filled=True)\nplt.title('Decision Tree when ccp_aplha=0.007')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:03:53.400566Z","iopub.execute_input":"2022-06-09T03:03:53.400844Z","iopub.status.idle":"2022-06-09T03:03:53.941982Z","shell.execute_reply.started":"2022-06-09T03:03:53.400818Z","shell.execute_reply":"2022-06-09T03:03:53.940724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_scores = pd.Series(dt.feature_importances_, index=X_train.columns).sort_values(ascending= False)\n\n# Creating a seaborn bar plot\n\nf, ax = plt.subplots(figsize=(30, 15))\nax = sns.barplot(x=feature_scores, y=feature_scores.index, data=df_train)\nax.set_title(\"Visualize importance of the features\")\nax.set_yticklabels(feature_scores.index)\nax.set_xlabel(\"Feature importance score\")\nax.set_ylabel(\"Features\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:03:53.94344Z","iopub.execute_input":"2022-06-09T03:03:53.943865Z","iopub.status.idle":"2022-06-09T03:03:54.825925Z","shell.execute_reply.started":"2022-06-09T03:03:53.943825Z","shell.execute_reply":"2022-06-09T03:03:54.825036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* <span style='color:brown'><b>‚¨ÜÔ∏èAbove bar plot shows importance of each feature while training Decision Tree. It can be seen that only 4 feature were important while importance score for all other features are zero.</b></span>\n\n* <span style='color:brown'><b> This is one of the advantages of Decision Tree that it can neglect features that are not important.</b></span>","metadata":{}},{"cell_type":"markdown","source":"***We can fine-tune Decision Tree to further select combination of best parameters***","metadata":{}},{"cell_type":"markdown","source":"### Fine-tuning using GridSearchCV","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:03:54.827028Z","iopub.execute_input":"2022-06-09T03:03:54.827316Z","iopub.status.idle":"2022-06-09T03:03:54.831244Z","shell.execute_reply.started":"2022-06-09T03:03:54.827288Z","shell.execute_reply":"2022-06-09T03:03:54.830425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"param = {'criterion': ['gini','entropy'], \n             'min_samples_leaf':[20,25,30,35,40],\n             'ccp_alpha':[0.002,0.005,0.007,0.009,0.01,0.02],\n             'min_samples_split':[2,3,4,5,6]}\n\ndt = DecisionTreeClassifier(class_weight='balanced',random_state=42)","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:03:54.832749Z","iopub.execute_input":"2022-06-09T03:03:54.833099Z","iopub.status.idle":"2022-06-09T03:03:54.842507Z","shell.execute_reply.started":"2022-06-09T03:03:54.833062Z","shell.execute_reply":"2022-06-09T03:03:54.841784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tune_dt = RandomizedSearchCV(dt, param, verbose=1,scoring='f1')\ntune_dt","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:03:54.843651Z","iopub.execute_input":"2022-06-09T03:03:54.843998Z","iopub.status.idle":"2022-06-09T03:03:54.861364Z","shell.execute_reply.started":"2022-06-09T03:03:54.843938Z","shell.execute_reply":"2022-06-09T03:03:54.860667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tune_dt.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:03:54.862437Z","iopub.execute_input":"2022-06-09T03:03:54.86305Z","iopub.status.idle":"2022-06-09T03:03:55.262496Z","shell.execute_reply.started":"2022-06-09T03:03:54.863019Z","shell.execute_reply":"2022-06-09T03:03:55.26162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Best hyper-parameter selected by GridSearchCV: \\n',tune_dt.best_params_)\nprint()\nprint('Best Score by GridSearchCV: ', tune_dt.best_score_)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:03:55.263806Z","iopub.execute_input":"2022-06-09T03:03:55.264226Z","iopub.status.idle":"2022-06-09T03:03:55.269879Z","shell.execute_reply.started":"2022-06-09T03:03:55.264184Z","shell.execute_reply":"2022-06-09T03:03:55.269017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Training Accuracy: ',tune_dt.score(X_train, y_train))\n\npred = tune_dt.predict(X_test)\nprint('Test Accuracy: ', accuracy_score(pred, y_test))\nprint('f1 Score: ', f1_score(pred, y_test))\nprint('Confusion Matrix:\\n ', confusion_matrix(pred, y_test))","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:03:55.271154Z","iopub.execute_input":"2022-06-09T03:03:55.271548Z","iopub.status.idle":"2022-06-09T03:03:55.288219Z","shell.execute_reply.started":"2022-06-09T03:03:55.27151Z","shell.execute_reply":"2022-06-09T03:03:55.287445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**üòîUnfortunately, Fine Tuning with RandomizedSearchCV couldn't generate as good result as tuning hyper parameters manually.**\n\nHowever at last, we got some familiarity with hyper-parameters of Decision Tree Classifier and some basic concepts of Machine learning such as overfitting, bias, variance and few more.","metadata":{}},{"cell_type":"markdown","source":"### Advantages:\n1. Compared to other algorithms decision trees requires less effort for data preparation during pre-processing.\n2. A decision tree does not require normalization of data.\n3. A decision tree does not require scaling of data as well.\n4. Missing values in the data also do NOT affect the process of building a decision tree to any considerable extent.\n5. A Decision tree model is very intuitive and easy to explain to technical teams as well as stakeholders.\n6. One Deicison tree model is train, time to give predictions on new data is very less.\n\n### Disadvantage:\n1. A small change in the data can cause a large change in the structure of the decision tree causing instability.\n2. For a Decision tree sometimes calculation can go far more complex compared to other algorithms.\n4. Decision tree often involves higher time to train the model.\n5. Decision tree training is relatively expensive as the complexity and time has taken are more.\n6. The Decision Tree algorithm is inadequate for applying regression and predicting continuous values.","metadata":{}},{"cell_type":"markdown","source":"\n\n**I Hope you found this notebook userful and enjoyable.**\n\n<span style=\"font-size:16px;\">üòäAlso, mention or discuss in comment section any other hyper-parameter of Decision Tree Classifier which is important but I haven't explained in this notebook.</span>\n\n**Thank you**","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\" style='color:black'>\n<b>Tip:</b> I would strongly recommend to go through the videos and article mentioned in the reference below to understand theory behind Decision Tree and some of the terms mentioned in this notebook like entropy and gini impurity; doing so will help to understand steps taken in this notebook more.\n</div>","metadata":{}},{"cell_type":"markdown","source":"***Reference:***\n\nhttps://www.youtube.com/watch?v=_L39rN6gz7Y\n\nhttps://www.youtube.com/watch?v=1IQOtJ4NI_0\n\nhttps://www.youtube.com/watch?v=5aIFgrrTqOw\n\nhttps://scikit-learn.org/stable/auto_examples/tree/plot_cost_complexity_pruning.html\n\nhttps://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html\n\nhttps://dhirajkumarblog.medium.com/top-5-advantages-and-disadvantages-of-decision-tree-algorithm-428ebd199d9a","metadata":{}}]}